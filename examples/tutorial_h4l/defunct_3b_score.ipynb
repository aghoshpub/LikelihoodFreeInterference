{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MadMiner particle physics tutorial\n",
    "\n",
    "# Part 3b: Training a score estimator\n",
    "\n",
    "Johann Brehmer, Felix Kling, Irina Espejo, and Kyle Cranmer 2018-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 3a of this tutorial we will finally train a neural network to estimate likelihood ratios. We assume that you have run part 1 and 2a of this tutorial. If, instead of 2a, you have run part 2b, you just have to load a different filename later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you've run the first tutorial before executing this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer import sampling\n",
    "from madminer.ml import ScoreEstimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MadMiner output\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Output of all other modules (e.g. matplotlib)\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make (unweighted) training and test samples with augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have all the information we need from the simulations. But the data is not quite ready to be used for machine learning. The `madminer.sampling` class `SampleAugmenter` will take care of the remaining book-keeping steps before we can train our estimators:\n",
    "\n",
    "First, it unweights the samples, i.e. for a given parameter vector `theta` (or a distribution `p(theta)`) it picks events `x` such that their distribution follows `p(x|theta)`. The selected samples will all come from the event file we have so far, but their frequency is changed -- some events will appear multiple times, some will disappear.\n",
    "\n",
    "Second, `SampleAugmenter` calculates all the augmented data (\"gold\") that is the key to our new inference methods. Depending on the specific technique, these are the joint likelihood ratio and / or the joint score. It saves all these pieces of information for the selected events in a set of numpy files that can easily be used in any machine learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:55 madminer.analysis    INFO    Loading data from data/delphes_data_shuffled.h5\n",
      "12:55 madminer.analysis    INFO    Found 1 parameters\n",
      "12:55 madminer.analysis    INFO    Did not find nuisance parameters\n",
      "12:55 madminer.analysis    INFO    Found 5 benchmarks, of which 5 physical\n",
      "12:55 madminer.analysis    INFO    Found 5 observables\n",
      "12:55 madminer.analysis    INFO    Found 1423725 events\n",
      "12:55 madminer.analysis    INFO      370705 signal events sampled from benchmark no-higgs\n",
      "12:55 madminer.analysis    INFO      353650 signal events sampled from benchmark sm\n",
      "12:55 madminer.analysis    INFO      348138 signal events sampled from benchmark 1.5pow4-higgs\n",
      "12:55 madminer.analysis    INFO      351232 signal events sampled from benchmark 2pow4-higgs\n",
      "12:55 madminer.analysis    INFO    Found morphing setup with 5 components\n",
      "12:55 madminer.analysis    INFO    Did not find nuisance morphing setup\n"
     ]
    }
   ],
   "source": [
    "sampler = SampleAugmenter('data/delphes_data_shuffled.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant `SampleAugmenter` function for local score estimators is `extract_samples_train_local()`. As in part 3a of the tutorial, for the argument `theta` you can use the helper functions `sampling.benchmark()`, `sampling.benchmarks()`, `sampling.morphing_point()`, `sampling.morphing_points()`, and `sampling.random_morphing_points()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:57 madminer.sampling    INFO    Extracting training sample for local score regression. Sampling and score evaluation according to sm\n",
      "12:57 madminer.sampling    INFO    Starting sampling serially\n",
      "12:57 madminer.sampling    INFO    Sampling from parameter point 1 / 1\n",
      "12:57 madminer.sampling    INFO    Effective number of samples: mean 198335.26851767677, with individual thetas ranging from 198335.2685176769 to 198335.2685176769\n"
     ]
    }
   ],
   "source": [
    "x, theta, t_xz, _ = sampler.sample_train_local(\n",
    "    theta=sampling.benchmark('sm'),\n",
    "    #n_samples=4 * 10**5, #100000,\n",
    "    n_samples= 10**6, # fewer than others\n",
    "    folder='./data/samples',\n",
    "    filename='train_score'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same data as in part 3a, so you only have to execute this if you haven't gone through tutorial 3a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:57 madminer.sampling    INFO    Extracting evaluation sample. Sampling according to sm\n",
      "12:57 madminer.sampling    INFO    Starting sampling serially\n",
      "12:57 madminer.sampling    INFO    Sampling from parameter point 1 / 1\n",
      "12:57 madminer.sampling    INFO    Effective number of samples: mean 66122.48847424473, with individual thetas ranging from 66122.48847424476 to 66122.48847424476\n"
     ]
    }
   ],
   "source": [
    "_ = sampler.sample_test(\n",
    "    theta=sampling.benchmark('sm'),\n",
    "    n_samples=5*10**5,\n",
    "    folder='./data/samples',\n",
    "    filename='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train score estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to build a neural network. Only this time, instead of the likelihood ratio itself, we will estimate the gradient of the log likelihood with respect to the theory parameters -- the score. To be precise, the output of the neural network is an estimate of the score at some reference parameter point, for instance the Standard Model. A neural network that estimates this \"local\" score can be used to calculate the Fisher information at that point. The estimated score can also be used as a machine learning version of Optimal Observables, and likelihoods can be estimated based on density estimation in the estimated score space. This method for likelihood ratio estimation is called SALLY, and there is a closely related version called SALLINO. Both are explained in [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013) and [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020).\n",
    "\n",
    "The central object for this is the `madminer.ml.ScoreEstimator` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ScoreEstimator(n_hidden=(50,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:57 madminer.ml          INFO    Starting training\n",
      "12:57 madminer.ml          INFO      Batch size:             128\n",
      "12:57 madminer.ml          INFO      Optimizer:              amsgrad\n",
      "12:57 madminer.ml          INFO      Epochs:                 50\n",
      "12:57 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "12:57 madminer.ml          INFO      Validation split:       0.25\n",
      "12:57 madminer.ml          INFO      Early stopping:         True\n",
      "12:57 madminer.ml          INFO      Scale inputs:           True\n",
      "12:57 madminer.ml          INFO      Shuffle labels          False\n",
      "12:57 madminer.ml          INFO      Samples:                all\n",
      "12:57 madminer.ml          INFO    Loading training data\n",
      "12:57 madminer.utils.vario INFO      Loading data/samples/x_train_score.npy into RAM\n",
      "12:57 madminer.utils.vario INFO      Loading data/samples/t_xz_train_score.npy into RAM\n",
      "12:57 madminer.ml          INFO    Found 1000000 samples with 1 parameters and 5 observables\n",
      "12:57 madminer.ml          INFO    Setting up input rescaling\n",
      "12:57 madminer.ml          INFO    Creating model\n",
      "12:57 madminer.ml          INFO    Training model\n",
      "12:57 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "12:58 madminer.utils.ml.tr INFO    Epoch   3: train loss  0.75008 (mse_score:  0.750)\n",
      "12:58 madminer.utils.ml.tr INFO               val. loss   0.79659 (mse_score:  0.797)\n",
      "13:00 madminer.utils.ml.tr INFO    Epoch   6: train loss  0.74840 (mse_score:  0.748)\n",
      "13:00 madminer.utils.ml.tr INFO               val. loss   0.79493 (mse_score:  0.795)\n",
      "13:01 madminer.utils.ml.tr INFO    Epoch   9: train loss  0.74750 (mse_score:  0.748)\n",
      "13:01 madminer.utils.ml.tr INFO               val. loss   0.79430 (mse_score:  0.794)\n",
      "13:02 madminer.utils.ml.tr INFO    Epoch  12: train loss  0.74687 (mse_score:  0.747)\n",
      "13:02 madminer.utils.ml.tr INFO               val. loss   0.79423 (mse_score:  0.794)\n",
      "13:03 madminer.utils.ml.tr INFO    Epoch  15: train loss  0.74652 (mse_score:  0.747)\n",
      "13:03 madminer.utils.ml.tr INFO               val. loss   0.79329 (mse_score:  0.793)\n",
      "13:05 madminer.utils.ml.tr INFO    Epoch  18: train loss  0.74596 (mse_score:  0.746)\n",
      "13:05 madminer.utils.ml.tr INFO               val. loss   0.79264 (mse_score:  0.793)\n"
     ]
    }
   ],
   "source": [
    "estimator.train(\n",
    "    method='sally',\n",
    "    x='data/samples/x_train_score.npy',\n",
    "    t_xz='data/samples/t_xz_train_score.npy',\n",
    ")\n",
    "\n",
    "estimator.save('models/sally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate score estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the SM score on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ScoreEstimator(n_hidden=(50,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.load('models/sally')\n",
    "\n",
    "t_hat = estimator.evaluate_score(\n",
    "    x='data/samples/x_test.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the estimated score and how it is related to the observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('data/samples/x_test.npy')\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "#for i in range(2):\n",
    "for i in range(1):\n",
    "    \n",
    "    ax = plt.subplot(1,2,i+1)\n",
    "\n",
    "    sc = plt.scatter(x[:,0], x[:,1], c=t_hat[:,i], s=25., cmap='viridis', vmin=-1., vmax=1.)\n",
    "    cbar = plt.colorbar(sc)\n",
    "\n",
    "    cbar.set_label(r'$\\hat{t}_' + str(i) + r'(x | \\theta_{ref})$')\n",
    "    plt.xlabel(r'$p_{T,j1}$ [GeV]')\n",
    "    plt.ylabel(r'$\\Delta \\phi_{jj}$ Sally')\n",
    "    plt.xlim(10.,300.)\n",
    "    plt.ylim(-3.15,3.15)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
